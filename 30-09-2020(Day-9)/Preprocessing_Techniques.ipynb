{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data in Python\n",
    "\n",
    "* NaN : not a number -- special floating-point value\n",
    "* Working with duplicates and missing values\n",
    "    * isnull()\n",
    "    * notnull()\n",
    "    * dropna()\n",
    "    * fillna()\n",
    "    * replace()\n",
    "* Dropping duplicate data\n",
    "* Which values should be replaced with missing values based on data identifying and eliminating outliers\n",
    "\n",
    "#### Identifying and Eliminating Outliers\n",
    "* Outliers are observations that are significantly different from other data points\n",
    "* Outliers can adversely affect the training process of a machine learning algorithm, resulting in a loss of accuracy.\n",
    "* Need to use the mathematical formula and retrieve the outlier data.\n",
    "\n",
    "     **interquartile range(IQR) = Q3(quantile(0.75)) ‚àí Q1(quantile(0.25))**\n",
    "     ![boxplot](boxplot.png)\n",
    "\n",
    "     \n",
    "* **Plotting**\n",
    "* **Saving**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing with scikit-learn\n",
    "# Preprocessing Techniques\n",
    "* Data Preprocessing is a technique that is used to convert the raw data into a clean data set\n",
    "\n",
    "### Data preprocessing steps\n",
    "\n",
    "\n",
    "* Data collecting from sources\n",
    "* Clean through unnecessary data\n",
    "* analyzing data/ Processing data\n",
    "\n",
    "    * will learn data preprocessing techniques with scikit-learn, one of the most popular frameworks used for industry data science\n",
    "    * The scikit-learn library includes tools for data preprocessing and data mining. It is imported in Python via the statement import sklearn.\n",
    "\n",
    "![ddd.PNG](ddd.PNG)\n",
    "\n",
    "### Data Imputation \n",
    "* if the dataset is missing too many values, we just don't use it\n",
    "*  if only a few of the values are missing, we can perform data imputation to substitute the missing data with some other value(s).\n",
    "* There are many different methods for data imputation\n",
    "    * Using the mean value\n",
    "    * Using the median value\n",
    "    * Using the most frequent value\n",
    "    * Filling in missing values with a constant\n",
    "    \n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "### 1.Standardizing Data\n",
    "\n",
    "\n",
    "* Data scientists will convert the data into a standard format to make it easier to understand.\n",
    "* The standard format refers to data that has 0 mean and unit variance (i.e. standard deviation = 1), and the process of    converting data into this format is called data standardization.\n",
    "* improve the performance of models\n",
    "* it rescales the data to have mean = 0 and varience(statistical measure that provides indicator of data's dispresion) = 1\n",
    "\n",
    "* Standardization rescales data so that it has a mean of 0 and a standard deviation of 1.\n",
    "* The formula for this is:  (ùë• ‚àí ùúá)/ùúé\n",
    "\n",
    "    * We subtract the mean (ùúá) from each value (x) and then divide by the standard deviation (ùúé)\n",
    "    \n",
    "![stddata.PNG](stddata.PNG)\n",
    "\n",
    "![std.PNG](std.PNG)\n",
    "\n",
    "    \n",
    "### 2. Data Range\n",
    "* Scale data by compressing it into a fixed range\n",
    "* One of the biggest use cases for this is compressing data into the range [0, 1]\n",
    "* MinMaxScaler \n",
    "![minmax.PNG](minmax.PNG)\n",
    "\n",
    "\n",
    "\n",
    "### 3. Robust Scaling\n",
    "* Deal with is outliers (data point that is significantly further away from the other data points)\n",
    "* Robustly scale the data, i.e. avoid being affected by outliers\n",
    "* Scaling by using data's median and Interquartile Range (IQR)\n",
    "* Here mean affected but median remains same\n",
    "* Subtract the median from each data value then scale to the IQR\n",
    "\n",
    "\n",
    "### 4. Normalizing Data\n",
    "\n",
    "* Want to scale the individual data observations (i.e. rows)\n",
    "* Rescales the data in smaller range -1.0 to 1.0 or 0.0 to 1.0.\n",
    "* Used in classification Problems and data mining \n",
    "* when clustering data we need to apply L2 normalization to each row\n",
    "* L2 normalization applied to a particular row of a data array \n",
    "* L2 norm of a row is just the square root of the sum of squared values for the row\n",
    "\n",
    "![norm.PNG](norm.PNG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
